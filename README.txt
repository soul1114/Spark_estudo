ğŸ“˜ README â€” Projeto PySpark
Este repositÃ³rio contÃ©m cinco notebooks organizados por etapas de um pipeline de dados com PySpark, desenvolvidos no Google Colab. Cada notebook aborda uma fase especÃ­fica: da ingestÃ£o Ã  otimizaÃ§Ã£o de dados.

ğŸ“‚ 1. leitura_escrita.ipynb
Objetivo: Leitura e escrita de arquivos com PySpark

Leitura de arquivos CSV e Parquet

InferÃªncia de esquema e tratamento de cabeÃ§alhos

Escrita de dados em formato Parquet

CriaÃ§Ã£o de tabelas temporÃ¡rias com Spark Catalog

ğŸ“‚ 2. preparacao.ipynb
Objetivo: PreparaÃ§Ã£o e transformaÃ§Ã£o de dados para machine learning

ExtraÃ§Ã£o de mÃªs e ano a partir de datas

IndexaÃ§Ã£o de variÃ¡veis categÃ³ricas (StringIndexer)

VetorizaÃ§Ã£o de features numÃ©ricas (VectorAssembler)

NormalizaÃ§Ã£o (Normalizer) e ReduÃ§Ã£o de dimensionalidade (PCA)

SeparaÃ§Ã£o em conjuntos de treino e teste

RegressÃ£o linear para prever comentÃ¡rios

ğŸ“‚ 3. tratamento.ipynb
Objetivo: Limpeza e prÃ©-processamento de dados

RemoÃ§Ã£o de valores nulos e duplicados

ConversÃ£o de tipos de dados

RenomeaÃ§Ã£o de colunas

CriaÃ§Ã£o de colunas auxiliares como Interaction

Salvamento de dados tratados

ğŸ“‚ 4. agragacao.ipynb
Objetivo: TÃ©cnicas de agregaÃ§Ã£o e anÃ¡lise exploratÃ³ria

Contagens, mÃ©dias, mÃ­nimos e mÃ¡ximos por categoria (Keyword)

CÃ¡lculo de variÃ¢ncia e mÃ©dia acumulativa com Window Functions

Contagem de valores Ãºnicos

AnÃ¡lises temporais por ano e mÃªs

ğŸ“‚ 5. otimizacao.ipynb
Objetivo: OtimizaÃ§Ã£o de joins e queries com particionamento

Uso de repartition e coalesce

CriaÃ§Ã£o de tabelas temporÃ¡rias e joins com spark.sql

AnÃ¡lise dos planos de execuÃ§Ã£o com .explain()

Filtros e projeÃ§Ãµes aplicadas antes do join

Salvamento do join otimizado em Parquet

âœ… Requisitos
Google Colab ou ambiente com PySpark instalado

Arquivos .csv e .parquet utilizados em cada notebook

Python 3.x